{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMbFWxQQyXE8D1rLHi7+9v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SethuGopalan/Listing/blob/master/ListingPrediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FRTJ9urXhMc"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from scipy.sparse import csr_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "data = pd.read_csv('/content/sample_data/listings.csv')\n",
        "\n",
        "# Fill missing values\n",
        "data['reviews_per_month'].fillna(0, inplace=True)\n",
        "data['last_review'].fillna('2020-01-01', inplace=True)\n",
        "\n",
        "# Select features and target\n",
        "features = ['neighbourhood_group', 'neighbourhood', 'latitude', 'longitude', 'room_type',\n",
        "            'minimum_nights', 'number_of_reviews', 'reviews_per_month',\n",
        "            'calculated_host_listings_count', 'availability_365']\n",
        "target = 'price'\n",
        "\n",
        "X = data[features]\n",
        "y = data[target]\n",
        "\n",
        "# Preprocess features\n",
        "numerical_features = ['latitude', 'longitude', 'minimum_nights', 'number_of_reviews',\n",
        "                      'reviews_per_month', 'calculated_host_listings_count', 'availability_365']\n",
        "categorical_features = ['neighbourhood_group', 'neighbourhood', 'room_type']\n",
        "\n",
        "# Standardize Numerical Features:\n",
        "# Standardization involves scaling the numerical features so they have a mean of 0 and a\n",
        "#  standard deviation of 1. This helps improve the performance and stability of machine learning models.\n",
        "#  The StandardScaler from sklearn.preprocessing is used for this purpose.\n",
        "\n",
        "# Standardize numerical features\n",
        "numerical_transformer = StandardScaler()\n",
        "\n",
        "# One-Hot Encode Categorical Features:\n",
        "# One-hot encoding converts categorical values into a format that can be provided to machine learning algorithms to do a better job in prediction.\n",
        "# It creates binary columns for each category, with a value of 1 indicating the presence of that category and 0 indicating its absence. The OneHotEncoder\n",
        "# from sklearn.preprocessing is used to perform this transformation. The parameter handle_unknown='ignore'\n",
        "# ensures that if there are any categories in the test data that were not seen during training, they are ignored rather than causing an error.\n",
        "\n",
        "# One-hot encode categorical features\n",
        "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "# Create a Preprocessor Using ColumnTransformer:\n",
        "# The ColumnTransformer allows you to apply different preprocessing steps to different columns in your dataset.\n",
        "# It combines the numerical and categorical transformations into a single transformation pipeline.\n",
        "\n",
        "# Numerical Transformer: Applied to the numerical features using StandardScaler.\n",
        "# Categorical Transformer: Applied to the categorical features using OneHotEncoder.\n",
        "\n",
        "# Create preprocessor\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "# Apply the Preprocessor to the Data:\n",
        "# The preprocessor.fit_transform(X) line applies the defined transformations to the dataset X. This transforms the numerical\n",
        "#  features using standardization and the categorical features using one-hot encoding.\n",
        "\n",
        "\n",
        "# Preprocess the data\n",
        "X_processed = preprocessor.fit_transform(X)\n",
        "\n",
        "# # X_train: The feature matrix for the training set.\n",
        "# X_test: The feature matrix for the testing set.\n",
        "# y_train: The target vector for the training set.\n",
        "# y_test: The target vector for the testing set.\n",
        "# X: The feature matrix (inputs) of your dataset. This includes all the columns you want to use to make predictions.\n",
        "# y: The target vector (output) of your dataset. This includes the column you want to predict.\n",
        "# test_size=0.2: This specifies the proportion of the dataset to include in the test split. Here, 0.2 means 20% of\n",
        "# the data will be used for testing, and the remaining 80% will be used for training.\n",
        "# random_state=42: This is a seed for the random number generator. Setting a random state ensures reproducibility,\n",
        "# meaning that every time you run the code with the same random state, you get the same split.\n",
        "\n",
        "# Example Scenario:\n",
        "# Suppose you have a dataset with 100 rows of data. The train_test_split function will randomly split this data into two sets:\n",
        "\n",
        "# Training Set: 80 rows (80% of the data) – X_train and y_train\n",
        "# Testing Set: 20 rows (20% of the data) – X_test and y_test\n"
      ],
      "metadata": {
        "id": "oS_JdpHiXpxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if isinstance(X_processed, csr_matrix):\n",
        "    X_processed = X_processed.toarray()"
      ],
      "metadata": {
        "id": "r_65Lt6JnbtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = y.to_numpy()"
      ],
      "metadata": {
        "id": "uR3_NnvCoLAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "tj5fCAipYA0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you preprocess data using tools like StandardScaler and OneHotEncoder, the resulting data can sometimes be in a sparse matrix format. A sparse matrix is a matrix in which most of the elements are zero. Sparse matrices are efficient for storing and processing large datasets with many zeros because they save memory and computational resources.\n",
        "Check if Data is Sparse:\n",
        "if isinstance(X_processed, csr_matrix):\n",
        "This line checks if X_processed is an instance of the csr_matrix class. csr_matrix stands for Compressed Sparse Row matrix, which is a specific type of sparse matrix provided by the SciPy library.\n",
        "Convert Sparse Matrix to Dense Array:\n",
        "X_processed = X_processed.toarray()\n",
        "If X_processed is a csr_matrix, this line converts it to a dense array (also known as a dense matrix). A dense array is a standard NumPy array where all values, including zeros, are explicitly stored."
      ],
      "metadata": {
        "id": "Ut6S2z44l3GD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LvaL3diKlzhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(32)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(32)\n"
      ],
      "metadata": {
        "id": "R0ye8zC4YGKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a simple neural network model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)  # Output layer for regression\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_dataset, epochs=50)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, mae = model.evaluate(test_dataset)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test MAE:\", mae)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(test_dataset)\n",
        "print(\"Predictions:\", predictions[:5])  # Print first 5 predictions\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKCLMWrHYJne",
        "outputId": "84d8f7e6-5acc-40b5-ba90-044bdcd685ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1210/1210 [==============================] - 4s 3ms/step - loss: 60657.3086 - mae: 77.0307\n",
            "Epoch 2/50\n",
            "1210/1210 [==============================] - 3s 2ms/step - loss: 57562.4688 - mae: 69.4678\n",
            "Epoch 3/50\n",
            "1210/1210 [==============================] - 3s 3ms/step - loss: 57158.8789 - mae: 69.0325\n",
            "Epoch 4/50\n",
            "1210/1210 [==============================] - 4s 3ms/step - loss: 56832.3828 - mae: 68.7707\n",
            "Epoch 5/50\n",
            "1210/1210 [==============================] - 4s 3ms/step - loss: 56543.8398 - mae: 68.5523\n",
            "Epoch 6/50\n",
            "1210/1210 [==============================] - 3s 2ms/step - loss: 56266.2617 - mae: 68.3588\n",
            "Epoch 7/50\n",
            "1210/1210 [==============================] - 3s 3ms/step - loss: 55998.4023 - mae: 68.2266\n",
            "Epoch 8/50\n",
            "1210/1210 [==============================] - 3s 2ms/step - loss: 55716.8867 - mae: 68.0904\n",
            "Epoch 9/50\n",
            "1210/1210 [==============================] - 3s 2ms/step - loss: 55426.9648 - mae: 67.9768\n",
            "Epoch 10/50\n",
            "1210/1210 [==============================] - 4s 3ms/step - loss: 55130.0469 - mae: 67.8496\n",
            "Epoch 11/50\n",
            "1210/1210 [==============================] - 3s 2ms/step - loss: 54826.9023 - mae: 67.7421\n",
            "Epoch 12/50\n",
            "1210/1210 [==============================] - 3s 3ms/step - loss: 54515.4961 - mae: 67.6435\n",
            "Epoch 13/50\n",
            "1210/1210 [==============================] - 4s 3ms/step - loss: 54189.2070 - mae: 67.5394\n",
            "Epoch 14/50\n",
            "1210/1210 [==============================] - 3s 3ms/step - loss: 53849.3047 - mae: 67.4177\n",
            "Epoch 15/50\n",
            "1210/1210 [==============================] - 3s 2ms/step - loss: 53505.9961 - mae: 67.3182\n",
            "Epoch 16/50\n",
            "1210/1210 [==============================] - 3s 3ms/step - loss: 53153.5703 - mae: 67.2212\n",
            "Epoch 17/50\n",
            "1210/1210 [==============================] - 3s 3ms/step - loss: 52796.8867 - mae: 67.1137\n",
            "Epoch 18/50\n",
            "1210/1210 [==============================] - 3s 3ms/step - loss: 52419.2656 - mae: 66.9939\n",
            "Epoch 19/50\n",
            "1210/1210 [==============================] - 5s 4ms/step - loss: 52053.3828 - mae: 66.8666\n",
            "Epoch 20/50\n",
            "1210/1210 [==============================] - 3s 3ms/step - loss: 51677.9258 - mae: 66.7585\n",
            "Epoch 21/50\n",
            "1210/1210 [==============================] - 4s 3ms/step - loss: 51320.5508 - mae: 66.6715\n",
            "Epoch 22/50\n",
            "1210/1210 [==============================] - 4s 4ms/step - loss: 50961.0547 - mae: 66.5328\n",
            "Epoch 23/50\n",
            "1210/1210 [==============================] - 3s 3ms/step - loss: 50618.4531 - mae: 66.4859\n",
            "Epoch 24/50\n",
            "1210/1210 [==============================] - 3s 2ms/step - loss: 50277.5156 - mae: 66.4042\n",
            "Epoch 25/50\n",
            "1210/1210 [==============================] - 3s 3ms/step - loss: 49946.7852 - mae: 66.3297\n",
            "Epoch 26/50\n",
            "1210/1210 [==============================] - 3s 2ms/step - loss: 49619.6914 - mae: 66.2692\n",
            "Epoch 27/50\n",
            "1210/1210 [==============================] - 4s 3ms/step - loss: 49281.0234 - mae: 66.2008\n",
            "Epoch 28/50\n",
            "1210/1210 [==============================] - 4s 3ms/step - loss: 48968.9492 - mae: 66.1402\n",
            "Epoch 29/50\n",
            "1210/1210 [==============================] - 2s 2ms/step - loss: 48667.3789 - mae: 66.1193\n",
            "Epoch 30/50\n",
            "1210/1210 [==============================] - 3s 3ms/step - loss: 48359.0938 - mae: 66.0578\n",
            "Epoch 31/50\n",
            "1210/1210 [==============================] - 3s 2ms/step - loss: 48066.8086 - mae: 66.0460\n",
            "Epoch 32/50\n",
            "1210/1210 [==============================] - 3s 2ms/step - loss: 47787.3867 - mae: 66.0015\n",
            "Epoch 33/50\n",
            "1210/1210 [==============================] - 4s 4ms/step - loss: 47528.1172 - mae: 66.0049\n",
            "Epoch 34/50\n",
            "1210/1210 [==============================] - 3s 3ms/step - loss: 47253.8867 - mae: 65.9770\n",
            "Epoch 35/50\n",
            "1210/1210 [==============================] - 3s 3ms/step - loss: 47008.0664 - mae: 65.9772\n",
            "Epoch 36/50\n",
            "1210/1210 [==============================] - 4s 3ms/step - loss: 46762.7969 - mae: 65.9576\n",
            "Epoch 37/50\n",
            "1210/1210 [==============================] - 3s 2ms/step - loss: 46535.6914 - mae: 65.9754\n",
            "Epoch 38/50\n",
            "1210/1210 [==============================] - 5s 4ms/step - loss: 46300.7852 - mae: 65.9560\n",
            "Epoch 39/50\n",
            "1210/1210 [==============================] - 3s 3ms/step - loss: 46077.0469 - mae: 65.9633\n",
            "Epoch 40/50\n",
            "1210/1210 [==============================] - 4s 3ms/step - loss: 45861.7617 - mae: 65.9743\n",
            "Epoch 41/50\n",
            "1210/1210 [==============================] - 6s 5ms/step - loss: 45655.1133 - mae: 65.9840\n",
            "Epoch 42/50\n",
            "1210/1210 [==============================] - 3s 3ms/step - loss: 45430.0508 - mae: 65.9306\n",
            "Epoch 43/50\n",
            "1210/1210 [==============================] - 5s 4ms/step - loss: 45263.9609 - mae: 65.9789\n",
            "Epoch 44/50\n",
            "1210/1210 [==============================] - 3s 3ms/step - loss: 45063.4102 - mae: 65.9423\n",
            "Epoch 45/50\n",
            "1210/1210 [==============================] - 3s 3ms/step - loss: 44866.9258 - mae: 65.9419\n",
            "Epoch 46/50\n",
            "1210/1210 [==============================] - 4s 3ms/step - loss: 44694.0195 - mae: 65.9259\n",
            "Epoch 47/50\n",
            "1210/1210 [==============================] - 3s 3ms/step - loss: 44513.7109 - mae: 65.9187\n",
            "Epoch 48/50\n",
            "1210/1210 [==============================] - 3s 2ms/step - loss: 44342.1992 - mae: 65.8990\n",
            "Epoch 49/50\n",
            "1210/1210 [==============================] - 4s 3ms/step - loss: 44163.6797 - mae: 65.8820\n",
            "Epoch 50/50\n",
            "1210/1210 [==============================] - 3s 3ms/step - loss: 44034.4258 - mae: 65.8702\n",
            "303/303 [==============================] - 1s 2ms/step - loss: 71138.3672 - mae: 76.1128\n",
            "Test Loss: 71138.3671875\n",
            "Test MAE: 76.11277770996094\n",
            "303/303 [==============================] - 1s 2ms/step\n",
            "Predictions: [[116.79138]\n",
            " [173.43144]\n",
            " [153.9025 ]\n",
            " [276.8338 ]\n",
            " [211.07025]]\n"
          ]
        }
      ]
    }
  ]
}